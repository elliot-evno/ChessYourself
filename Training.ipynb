{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training of a transformer to play chess. Most effort went into variable naming tbh.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wQanZXmCWMiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets torch"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "mVQ7_PHNy1PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "yU6NoZ0sNP8B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"conacts/stockfish_dataset\")\n",
        "ds"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729705347172
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMq7uB44y1PP",
        "outputId": "410bd8d6-78e2-4295-ee6a-a87ec4e6bfce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir -p stockfish_data\n",
        "\n",
        "# Save the dataset to the local directory\n",
        "#ds.save_to_disk(\"./stockfish_data\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729597223064
        },
        "id": "a4ELxiZ6y1PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = ds['train']\n",
        "games = train\n",
        "\n",
        "#Dataset handling\n",
        "def extract_moves(game_data):\n",
        "    if isinstance(game_data, str):\n",
        "        return game_data.split()\n",
        "    elif isinstance(game_data, dict) and 'moves' in game_data:\n",
        "        return game_data['moves'].split()\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpected game_data type: {type(game_data)}. Expected a dictionary with a 'moves' key or a string.\")\n",
        "\n",
        "\n",
        "\n",
        "unique_moves = set()\n",
        "\n",
        "# progress bar is nice\n",
        "for game in tqdm(games, desc=\"Processing games\"):\n",
        "    try:\n",
        "        moves = extract_moves(game)\n",
        "        unique_moves.update(moves)\n",
        "        len(unique_moves)\n",
        "\n",
        "    except TypeError as e:\n",
        "        print(f\"Skipping invalid game data: {e}\")\n",
        "\n",
        "# Build vocab\n",
        "vocab = {move: idx for idx, move in enumerate(unique_moves)}\n",
        "len(vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvVL6q-i8g-r",
        "outputId": "9cf08537-5833-4008-eeca-2b2d5c80c9ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing games:   1%|‚ñè         | 99998/7896048 [00:06<08:04, 16091.69it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8243"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save vocab as json\n",
        "with open('vocab.json', 'w') as f:\n",
        "  json.dump(vocab, f)"
      ],
      "metadata": {
        "id": "t2p7KCxy_Ck5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_moves(moves, vocab):\n",
        "    return [vocab[move] for move in moves]\n",
        "\n",
        "#Example:\n",
        "tokenized_moves = tokenize_moves(moves, vocab)\n",
        "print(tokenized_moves[:10])\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8243\n",
            "[1305, 2983, 1419, 8238, 6696, 2636, 4103, 4989, 4337, 3201]\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729708886316
        },
        "id": "RuQ9YjCny1PY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e279a60-f11b-468d-9833-d76cafddea2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(tokenized_moves, seq_len):\n",
        "    data = []\n",
        "    for i in range(len(tokenized_moves) - seq_len):\n",
        "        src = torch.tensor(tokenized_moves[i:i+seq_len], dtype=torch.long)\n",
        "        tgt = torch.tensor(tokenized_moves[i+1:i+seq_len+1], dtype=torch.long)\n",
        "        data.append((src, tgt))\n",
        "    return data"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729708901963
        },
        "id": "0e-3SNBky1PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuneable hyperparams\n",
        "D_MODEL = 512\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 8\n",
        "DIM_FEEDFORWARD = 4096\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "STEP_SIZE = 5\n",
        "GAMMA = 0.1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "SEQ_LEN = 15\n",
        "TRAIN_SPLIT = 0.8\n",
        "ACCUMULATION_STEPS = 4\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=DROPOUT, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class PreNormTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=DIM_FEEDFORWARD, dropout=DROPOUT):\n",
        "        super(PreNormTransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        #pre layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # gelu activation\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, src):\n",
        "        src2 = self.self_attn(self.norm1(src), self.norm1(src), self.norm1(src))[0]\n",
        "        src = src + self.alpha * self.dropout(src2)\n",
        "\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(self.norm2(src)))))\n",
        "        src = src + self.alpha * self.dropout(src2)\n",
        "\n",
        "        return src\n",
        "\n",
        "class ChessTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT):\n",
        "        super(ChessTransformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        transformer_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=DIM_FEEDFORWARD, dropout=dropout)\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layers, num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer(src)\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Init model, loss function, and optimizer with wd\n",
        "model = ChessTransformer(vocab_size=len(vocab), d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "#lr scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "def create_data_loader(data, batch_size=BATCH_SIZE):\n",
        "    srcs, tgts = zip(*data)\n",
        "    srcs = torch.stack(srcs)\n",
        "    tgts = torch.stack(tgts)\n",
        "    dataset = TensorDataset(srcs, tgts)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_data = prepare_data(tokenized_moves, seq_len=SEQ_LEN)\n",
        "\n",
        "#Valid/train Split\n",
        "train_size = int(TRAIN_SPLIT * len(train_data))\n",
        "valid_size = len(train_data) - train_size\n",
        "train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
        "\n",
        "# Prepare dls\n",
        "train_loader = create_data_loader(train_data, batch_size=BATCH_SIZE)\n",
        "valid_loader = create_data_loader(valid_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, valid_loader, epochs=EPOCHS):\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_predictions = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src)\n",
        "            loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "            loss = loss / ACCUMULATION_STEPS\n",
        "            loss.backward()  # backprop\n",
        "\n",
        "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
        "                # gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "                optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "            #accuracy on training set\n",
        "            predictions = output.argmax(dim=-1)\n",
        "            correct_train_predictions += (predictions == tgt).sum().item()\n",
        "            total_train_predictions += tgt.numel()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_accuracy = correct_train_predictions / total_train_predictions\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_valid_loss = 0\n",
        "        correct_valid_predictions = 0\n",
        "        total_valid_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                src, tgt = batch\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "                output = model(src)\n",
        "                loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "                total_valid_loss += loss.item()\n",
        "\n",
        "\n",
        "                predictions = output.argmax(dim=-1)\n",
        "                correct_valid_predictions += (predictions == tgt).sum().item()\n",
        "                total_valid_predictions += tgt.numel()\n",
        "\n",
        "        avg_valid_loss = total_valid_loss / len(valid_loader)\n",
        "        valid_accuracy = correct_valid_predictions / total_valid_predictions\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
        "\n",
        "# train\n",
        "train_model(model, train_loader, valid_loader, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "_vzucZrgz8Pd",
        "outputId": "1797e5e3-5281-4378-83c2-15abfe9abb3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 9.2072, Train Accuracy: 0.0000, Valid Loss: 9.1963, Valid Accuracy: 0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d14765aad94c>\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-d14765aad94c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, epochs)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mACCUMULATION_STEPS\u001b[0m  \u001b[0;31m# Normalize loss for accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mACCUMULATION_STEPS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test set\n",
        "test_data = ds['test']\n",
        "\n",
        "test_loader = create_data_loader(test_data, batch_size=64)\n",
        "\n",
        "\n",
        "#Evaulation on test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    correct_test_predictions = 0\n",
        "    total_test_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            output = model(src)\n",
        "            loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # calculating accuracy on test set\n",
        "            predictions = output.argmax(dim=-1)\n",
        "            correct_test_predictions += (predictions == tgt).sum().item()\n",
        "            total_test_predictions += tgt.numel()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    test_accuracy = correct_test_predictions / total_test_predictions\n",
        "\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "ahxYX7Sm64yn",
        "outputId": "58320247-d9a9-429d-e180-e4c31b480b97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-240e30d4caea>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Prepare data loaders for training, validation, and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d14765aad94c>\u001b[0m in \u001b[0;36mcreate_data_loader\u001b[0;34m(data, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0msrcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0msrcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtgts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"chess_transformer.pth\")"
      ],
      "metadata": {
        "id": "Qe4F_Tf01oMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}