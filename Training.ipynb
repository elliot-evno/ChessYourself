{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training of a transformer to play chess.",
        "\"\"\""
      ],
      "metadata": {
        "id": "wQanZXmCWMiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets torch"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "mVQ7_PHNy1PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "yU6NoZ0sNP8B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"conacts/stockfish_dataset\")\n",
        "ds"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729705347172
        },
        "id": "VMq7uB44y1PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir -p stockfish_data\n",
        "\n",
        "# Save the dataset to the local directory\n",
        "#ds.save_to_disk(\"./stockfish_data\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729597223064
        },
        "id": "a4ELxiZ6y1PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = ds['train']\n",
        "games = train\n",
        "\n",
        "#Dataset handling\n",
        "def extract_moves(game_data):\n",
        "    if isinstance(game_data, str):\n",
        "        return game_data.split()\n",
        "    elif isinstance(game_data, dict) and 'moves' in game_data:\n",
        "        return game_data['moves'].split()\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpected game_data type: {type(game_data)}. Expected a dictionary with a 'moves' key or a string.\")\n",
        "\n",
        "\n",
        "\n",
        "unique_moves = set()\n",
        "\n",
        "# progress bar is nice\n",
        "for game in tqdm(games, desc=\"Processing games\"):\n",
        "    try:\n",
        "        moves = extract_moves(game)\n",
        "        unique_moves.update(moves)\n",
        "        len(unique_moves)\n",
        "\n",
        "    except TypeError as e:\n",
        "        print(f\"Skipping invalid game data: {e}\")\n",
        "\n",
        "# Build vocab\n",
        "vocab = {move: idx for idx, move in enumerate(unique_moves)}\n",
        "len(vocab)\n"
      ],
      "metadata": {
        "id": "dvVL6q-i8g-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save vocab as json\n",
        "with open('vocab.json', 'w') as f:\n",
        "  json.dump(vocab, f)"
      ],
      "metadata": {
        "id": "t2p7KCxy_Ck5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_moves(moves, vocab):\n",
        "    return [vocab[move] for move in moves]\n",
        "\n",
        "#Example:\n",
        "tokenized_moves = tokenize_moves(moves, vocab)\n",
        "print(tokenized_moves[:10])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729708886316
        },
        "id": "RuQ9YjCny1PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(tokenized_moves, seq_len):\n",
        "    data = []\n",
        "    for i in range(len(tokenized_moves) - seq_len):\n",
        "        src = torch.tensor(tokenized_moves[i:i+seq_len], dtype=torch.long)\n",
        "        tgt = torch.tensor(tokenized_moves[i+1:i+seq_len+1], dtype=torch.long)\n",
        "        data.append((src, tgt))\n",
        "    return data"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729708901963
        },
        "id": "0e-3SNBky1PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuneable hyperparams\n",
        "D_MODEL = 512\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 8\n",
        "DIM_FEEDFORWARD = 4096\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 1e-4\n",
        "STEP_SIZE = 5\n",
        "GAMMA = 0.1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "SEQ_LEN = 15\n",
        "TRAIN_SPLIT = 0.8\n",
        "ACCUMULATION_STEPS = 4\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=DROPOUT, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class PreNormTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=DIM_FEEDFORWARD, dropout=DROPOUT):\n",
        "        super(PreNormTransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        #pre layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # gelu activation\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, src):\n",
        "        src2 = self.self_attn(self.norm1(src), self.norm1(src), self.norm1(src))[0]\n",
        "        src = src + self.alpha * self.dropout(src2)\n",
        "\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(self.norm2(src)))))\n",
        "        src = src + self.alpha * self.dropout(src2)\n",
        "\n",
        "        return src\n",
        "\n",
        "class ChessTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT):\n",
        "        super(ChessTransformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        transformer_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=DIM_FEEDFORWARD, dropout=dropout)\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layers, num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer(src)\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Init model, loss function, and optimizer with wd\n",
        "model = ChessTransformer(vocab_size=len(vocab), d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "#lr scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "def create_data_loader(data, batch_size=BATCH_SIZE):\n",
        "    srcs, tgts = zip(*data)\n",
        "    srcs = torch.stack(srcs)\n",
        "    tgts = torch.stack(tgts)\n",
        "    dataset = TensorDataset(srcs, tgts)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_data = prepare_data(tokenized_moves, seq_len=SEQ_LEN)\n",
        "\n",
        "#Valid/train Split\n",
        "train_size = int(TRAIN_SPLIT * len(train_data))\n",
        "valid_size = len(train_data) - train_size\n",
        "train_data, valid_data = random_split(train_data, [train_size, valid_size])\n",
        "\n",
        "# Prepare dls\n",
        "train_loader = create_data_loader(train_data, batch_size=BATCH_SIZE)\n",
        "valid_loader = create_data_loader(valid_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, valid_loader, epochs=EPOCHS):\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        correct_train_predictions = 0\n",
        "        total_train_predictions = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src)\n",
        "            loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "            loss = loss / ACCUMULATION_STEPS\n",
        "            loss.backward()  # backprop\n",
        "\n",
        "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
        "                # gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "                optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "            #accuracy on training set\n",
        "            predictions = output.argmax(dim=-1)\n",
        "            correct_train_predictions += (predictions == tgt).sum().item()\n",
        "            total_train_predictions += tgt.numel()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_accuracy = correct_train_predictions / total_train_predictions\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_valid_loss = 0\n",
        "        correct_valid_predictions = 0\n",
        "        total_valid_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                src, tgt = batch\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "                output = model(src)\n",
        "                loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "                total_valid_loss += loss.item()\n",
        "\n",
        "\n",
        "                predictions = output.argmax(dim=-1)\n",
        "                correct_valid_predictions += (predictions == tgt).sum().item()\n",
        "                total_valid_predictions += tgt.numel()\n",
        "\n",
        "        avg_valid_loss = total_valid_loss / len(valid_loader)\n",
        "        valid_accuracy = correct_valid_predictions / total_valid_predictions\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
        "\n",
        "# train\n",
        "train_model(model, train_loader, valid_loader, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "id": "_vzucZrgz8Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test set\n",
        "test_data = ds['test']\n",
        "\n",
        "test_loader = create_data_loader(test_data, batch_size=64)\n",
        "\n",
        "\n",
        "#Evaulation on test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    correct_test_predictions = 0\n",
        "    total_test_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            output = model(src)\n",
        "            loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # calculating accuracy on test set\n",
        "            predictions = output.argmax(dim=-1)\n",
        "            correct_test_predictions += (predictions == tgt).sum().item()\n",
        "            total_test_predictions += tgt.numel()\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "    test_accuracy = correct_test_predictions / total_test_predictions\n",
        "\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "ahxYX7Sm64yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"chess_transformer.pth\")"
      ],
      "metadata": {
        "id": "Qe4F_Tf01oMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
